{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import itertools\n",
        "from google.colab import files\n",
        "\n",
        "#CFG Rules File\n",
        "uploaded = files.upload()\n",
        "input_path = next(iter(uploaded.keys()))\n",
        "\n",
        "# Load CFG Rules\n",
        "def load_cfg_from_file(path):\n",
        "    rules = []\n",
        "    with open(path, 'r') as f:\n",
        "        rule_lines = [line.strip().strip(',') for line in f if line.strip()]\n",
        "    rules = []\n",
        "    for line in rule_lines:\n",
        "        match = re.match(r\"\\('([^']+)',\\s*\\[([^\\]]+)\\]\\)\", line)\n",
        "        if match:\n",
        "            lhs = match.group(1)\n",
        "            rhs = [sym.strip().strip(\"'\") for sym in match.group(2).split(',')]\n",
        "            rules.append((lhs, rhs))\n",
        "    return rules"
      ],
      "metadata": {
        "id": "AiK1JgmaR1FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNF Converter\n",
        "class CNFConverter:\n",
        "    def __init__(self, rules):\n",
        "        self.original_rules = rules\n",
        "        self.new_rules = []\n",
        "        self.term_map = {}\n",
        "        self.counter = itertools.count(1)\n",
        "\n",
        "    def _get_new_nonterminal(self, base='X'):\n",
        "        return f'{base}{next(self.counter)}'\n",
        "\n",
        "    def convert(self):\n",
        "        rules = self.original_rules[:]\n",
        "        updated_rules = []\n",
        "\n",
        "        for lhs, rhs in rules:\n",
        "            new_rhs = []\n",
        "            for sym in rhs:\n",
        "                if len(rhs) > 1 and not re.fullmatch(r\"[A-Z$]+\", sym):\n",
        "                    if sym not in self.term_map:\n",
        "                        nt = self._get_new_nonterminal('T')\n",
        "                        self.term_map[sym] = nt\n",
        "                        self.new_rules.append((nt, [sym]))\n",
        "                    new_rhs.append(self.term_map[sym])\n",
        "                else:\n",
        "                    new_rhs.append(sym)\n",
        "            updated_rules.append((lhs, new_rhs))\n",
        "\n",
        "        for lhs, rhs in updated_rules:\n",
        "            while len(rhs) > 2:\n",
        "                first, second, *rest = rhs\n",
        "                new_nt = self._get_new_nonterminal()\n",
        "                self.new_rules.append((lhs, [first, new_nt]))\n",
        "                lhs = new_nt\n",
        "                rhs = [second] + rest\n",
        "            self.new_rules.append((lhs, rhs))\n",
        "\n",
        "        return self.new_rules\n",
        "\n",
        "def write_cnf_to_file(rules, path):\n",
        "    with open(path, 'w') as f:\n",
        "        for lhs, rhs in rules:\n",
        "            f.write(f\"('{lhs}', {rhs}),\\n\")\n",
        "\n",
        "cfg_rules = load_cfg_from_file(input_path)\n",
        "converter = CNFConverter(cfg_rules)\n",
        "cnf_rules = converter.convert()\n",
        "output_path = 'cnf_rules.txt'\n",
        "write_cnf_to_file(cnf_rules, output_path)\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "74SFLEBsR7m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GZpx6sA9pEZ"
      },
      "outputs": [],
      "source": [
        "#CYK parser\n",
        "\n",
        "def cyk_parse(words, grammar, start_symbol='S'):\n",
        "    \"\"\"\n",
        "    :param words: list of words in the input sentence\n",
        "    :param grammar: list of (lhs, rhs) rules in CNF, e.g. ('S', ['NP', 'VP'])\n",
        "    :param start_symbol: the start symbol of the grammar\n",
        "    :return: (table, back) where table[l][s][X] = True iff X derives words[s:s+l]\n",
        "    \"\"\"\n",
        "    n = len(words)\n",
        "    # Extract all non-terminals\n",
        "    nonterminals = sorted({lhs for lhs, rhs in grammar})\n",
        "    nt_index = {nt: i for i, nt in enumerate(nonterminals)}\n",
        "\n",
        "    # Table for P[l][s][v] as in pseudocode (using a dict for efficiency)\n",
        "    table = [[dict() for _ in range(n)] for _ in range(n+1)]\n",
        "    back = [[dict() for _ in range(n)] for _ in range(n+1)]\n",
        "\n",
        "    # Preprocess rules\n",
        "    unary_rules = {}  # a -> [A]\n",
        "    binary_rules = [] # (A, B, C)\n",
        "\n",
        "    for lhs, rhs in grammar:\n",
        "        if len(rhs) == 1:\n",
        "            unary_rules.setdefault(rhs[0], []).append(lhs)\n",
        "        elif len(rhs) == 2:\n",
        "            binary_rules.append((lhs, rhs[0], rhs[1]))\n",
        "\n",
        "    # Initialize for length 1 substrings\n",
        "    for s in range(n):\n",
        "        for A in unary_rules.get(words[s], []):\n",
        "            table[1][s][A] = True\n",
        "            back[1][s][A] = (words[s],)\n",
        "\n",
        "    # Main CYK loop\n",
        "    for l in range(2, n+1):  # Length of span\n",
        "        for s in range(n-l+1):  # Start of span\n",
        "            for p in range(1, l):  # Partition\n",
        "                left_cell = table[p][s]\n",
        "                right_cell = table[l-p][s+p]\n",
        "                for A, B, C in binary_rules:\n",
        "                    if B in left_cell and C in right_cell:\n",
        "                        table[l][s][A] = True\n",
        "                        back[ l ][ s ][ A ] = (p, B, C)\n",
        "\n",
        "    # Is the input in the language?\n",
        "    accepted = start_symbol in table[n][0]\n",
        "\n",
        "    return accepted, table, back, nonterminals\n",
        "\n",
        "def build_tree(back, l, s, symbol):\n",
        "    # Recursively reconstruct the parse tree\n",
        "    entry = back[l][s][symbol]\n",
        "    if isinstance(entry, tuple) and len(entry) == 1:\n",
        "        return (symbol, entry[0])  # Terminal\n",
        "    else:\n",
        "        p, B, C = entry\n",
        "        left = build_tree(back, p, s, B)\n",
        "        right = build_tree(back, l-p, s+p, C)\n",
        "        return (symbol, left, right)\n",
        "\n",
        "from nltk import Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Example\n",
        "\n",
        "# Load grammar from cnf_rules.txt\n",
        "def load_cnf_rules(filepath):\n",
        "    rules = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().strip(',')\n",
        "            if line:\n",
        "                try:\n",
        "                    lhs, rhs_str = line.split(', [')\n",
        "                    lhs = lhs.strip(\"('\").strip(\"'\")\n",
        "                    rhs = [sym.strip().strip(\"'\") for sym in rhs_str.strip(\"])\").split(',')]\n",
        "                    rules.append((lhs, rhs))\n",
        "                except ValueError:\n",
        "                    print(f\"Skipping invalid line: {line}\")\n",
        "    return rules\n",
        "\n",
        "# Load the grammar and then append the rule\n",
        "grammar = load_cnf_rules('cnf_rules (10).txt')\n",
        "grammar.append(('CC', ['CC']))\n",
        "grammar.append(('IN', ['IN']))\n",
        "grammar.append(('VBG', ['VBG']))\n",
        "grammar.append(('CD', ['CD']))   # Cardinal number\n",
        "grammar.append(('DT', ['DT']))   # Determiner\n",
        "grammar.append(('EX', ['EX']))   # Existential there\n",
        "grammar.append(('FW', ['FW']))   # Foreign word\n",
        "grammar.append(('JJ', ['JJ']))   # Adjective\n",
        "grammar.append(('JJR', ['JJR'])) # Adjective, comparative\n",
        "grammar.append(('JJS', ['JJS'])) # Adjective, superlative\n",
        "grammar.append(('LS', ['LS']))   # List item marker\n",
        "grammar.append(('MD', ['MD']))   # Modal\n",
        "grammar.append(('NN', ['NN']))   # Noun, singular or mass\n",
        "grammar.append(('NNS', ['NNS'])) # Noun, plural\n",
        "grammar.append(('NNP', ['NNP'])) # Proper noun, singular\n",
        "grammar.append(('NNPS', ['NNPS'])) # Proper noun, plural\n",
        "grammar.append(('PDT', ['PDT'])) # Predeterminer\n",
        "grammar.append(('POS', ['POS'])) # Possessive ending\n",
        "grammar.append(('PRP', ['PRP'])) # Personal pronoun\n",
        "grammar.append(('PP', ['PP']))   # Possessive pronoun\n",
        "grammar.append(('RB', ['RB']))   # Adverb\n",
        "grammar.append(('RBR', ['RBR'])) # Adverb, comparative\n",
        "grammar.append(('RBS', ['RBS'])) # Adverb, superlative\n",
        "grammar.append(('RP', ['RP']))   # Particle\n",
        "grammar.append(('SYM', ['SYM'])) # Symbol\n",
        "grammar.append(('TO', ['TO']))   # to\n",
        "grammar.append(('UH', ['UH']))   # Interjection\n",
        "grammar.append(('VB', ['VB']))   # Verb, base form\n",
        "grammar.append(('VBD', ['VBD'])) # Verb, past tense\n",
        "grammar.append(('VBN', ['VBN'])) # Verb, past participle\n",
        "grammar.append(('VBP', ['VBP'])) # Verb, non-3rd person singular present\n",
        "grammar.append(('VBZ', ['VBZ'])) # Verb, 3rd person singular present\n",
        "grammar.append(('WDT', ['WDT'])) # wh-determiner\n",
        "grammar.append(('WP', ['WP']))   # wh-pronoun\n",
        "grammar.append(('WP$', ['WP$'])) # Possessive wh-pronoun\n",
        "grammar.append(('WRB', ['WRB'])) # wh-adverb\n",
        "grammar.append(('S', ['NP', 'VP']))\n",
        "grammar.append(('NP', ['DT', 'NN']))\n",
        "grammar.append(('VP', ['VBZ', 'NP']))\n",
        "grammar.append(('VP', ['VBZ', 'JJ']))\n",
        "grammar.append(('PP', ['IN', 'NP']))\n",
        "grammar.append(('NP', ['NN']))\n",
        "grammar.append(('NP', ['PRP']))\n",
        "grammar.append(('VP', ['VBD', 'PP']))\n",
        "grammar.append(('VP', ['VB', 'ADJP']))\n",
        "grammar.append(('ADJP', ['JJ']))\n",
        "grammar.append(('PRP$', ['PRP$']))\n",
        "\n"
      ],
      "metadata": {
        "id": "z5zaIYBMJdS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy\n"
      ],
      "metadata": {
        "id": "3PfCQQjAR8Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagger\n",
        "import sys\n",
        "import subprocess\n",
        "import spacy\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Ensure English model is installed\n",
        "try:\n",
        "    spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"English model not found. Downloading...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Universal POS tags to CFG map (adaptable for different languages)\n",
        "cfg_map = {\n",
        "    \"NOUN\": \"N\",\n",
        "    \"PROPN\": \"N\",\n",
        "    \"PRON\": \"Pro\",\n",
        "    \"VERB\": \"V\",\n",
        "    \"AUX\": \"V\",\n",
        "    \"ADJ\": \"Adj\",\n",
        "    \"ADV\": \"Adv\",\n",
        "    \"DET\": \"D\",\n",
        "    \"ADP\": \"P\",\n",
        "    \"CCONJ\": \"Conj\",\n",
        "    \"SCONJ\": \"Conj\",\n",
        "    \"PART\": \"Part\",\n",
        "    \"INTJ\": \"Intj\",\n",
        "    \"NUM\": \"Num\",\n",
        "    \"PUNCT\": \".\",\n",
        "}\n",
        "\n",
        "text = input(\"Enter a sentence: \")\n",
        "\n",
        "print(f\"\\nProcessing in English:\")\n",
        "\n",
        "doc = nlp(text)\n",
        "sentence = []\n",
        "print(\"\\nCFG-style POS Tags:\\n\")\n",
        "for token in doc:\n",
        "    tag = cfg_map.get(token.tag_, token.tag_)\n",
        "    sentence.append(tag)\n",
        "\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJUSc9nmVo8k",
        "outputId": "af9788c0-38f8-4532-cc72-aad35ebd9350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: the john eats he\n",
            "\n",
            "Processing in English:\n",
            "\n",
            "CFG-style POS Tags:\n",
            "\n",
            "the             → DT\n",
            "john            → NNP\n",
            "eats            → VBZ\n",
            "he              → PRP\n",
            "['DT', 'NNP', 'VBZ', 'PRP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Run\n",
        "accepted, table, back, nts = cyk_parse(sentence, grammar, start_symbol='S')\n",
        "if accepted:\n",
        "    print(\"Grammatically Correct!\")\n",
        "    tree = build_tree(back, len(sentence), 0, 'S')\n",
        "    print(\"Parse tree:\", tree)\n",
        "else:\n",
        "    print(\"Grammatically Incorrect!\")\n",
        "for l in range(1, len(sentence)+1):\n",
        "    for s in range(len(sentence)-l+1):\n",
        "        print(f\"Span length {l}, start {s}: {table[l][s]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D2wASTLLL8A",
        "outputId": "5281b172-0e06-407c-842f-5e4f05b164cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammatically Correct!\n",
            "Parse tree: ('S', ('NP', ('DT', 'DT'), ('NNP', 'NNP')), ('VP', ('VBZ', 'VBZ'), ('NP', 'PRP')))\n",
            "Span length 1, start 0: {'NP': True, 'DT': True}\n",
            "Span length 1, start 1: {'NP': True, 'NNP': True}\n",
            "Span length 1, start 2: {'VP': True, 'VBZ': True}\n",
            "Span length 1, start 3: {'NP': True, 'PRP': True}\n",
            "Span length 2, start 0: {'NP': True}\n",
            "Span length 2, start 1: {'S': True, 'NP': True, 'X10': True, 'X47': True, 'X54': True}\n",
            "Span length 2, start 2: {'VP': True}\n",
            "Span length 3, start 0: {'NP': True, 'X18': True, 'X53': True, 'X58': True, 'S': True, 'X10': True, 'X47': True, 'X54': True}\n",
            "Span length 3, start 1: {'S': True, 'NP': True, 'X10': True, 'X47': True, 'X54': True}\n",
            "Span length 4, start 0: {'NP': True, 'X18': True, 'X53': True, 'X58': True, 'S': True, 'X10': True, 'X47': True, 'X54': True}\n"
          ]
        }
      ]
    }
  ]
}